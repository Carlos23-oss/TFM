# -*- coding: utf-8 -*-
"""TFM1_ModeloDeCine_Grupo1_RECO_(1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dtnB5gUZOJsHhF8r7tYJMzJzpXjDGxxx

# TFM Modelo de cine: Recomendador  | Grupo 1 | Carlos Onrubia y Guillem Balagu√©

#1. Librer√≠as
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import polars as pl
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn import model_selection
import matplotlib.pyplot as plt
from typing import List, Sequence, Tuple

#pip install -U faiss-cpu
import faiss

import kagglehub
import dask.dataframe as dd
#pip install dask-sql --no-build-isolation
import dask.dataframe as dd
from dask_sql import Context
from dask.distributed import Client
import time
import gc
import pyarrow as pa
import pyarrow.parquet as pq
import os
import shutil
import ast
import seaborn as sns
import pandas as pd
import torch

import warnings
warnings.simplefilter(action = 'ignore')


import sklearn
from sklearn import set_config

from sklearn.tree import DecisionTreeClassifier

# transformers
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder

# pipelines
from sklearn.pipeline import Pipeline, FunctionTransformer
from sklearn.compose import ColumnTransformer

from sklearn.metrics import accuracy_score
from sklearn.feature_extraction import FeatureHasher
import scipy.sparse as sp

from sentence_transformers import SentenceTransformer, models, util
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import polars as pl
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn import model_selection
import matplotlib.pyplot as plt
from typing import List, Sequence, Tuple

#pip install -U faiss-cpu
import faiss

import kagglehub
import dask.dataframe as dd
#pip install dask-sql --no-build-isolation
import dask.dataframe as dd
from dask_sql import Context
from dask.distributed import Client
import time
import gc
import pyarrow as pa
import pyarrow.parquet as pq
import os
import shutil
import ast
import seaborn as sns
import pandas as pd
import torch

import warnings
warnings.simplefilter(action = 'ignore')


import sklearn
from sklearn import set_config

from sklearn.tree import DecisionTreeClassifier

# transformers
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, OneHotEncoder

# pipelines
from sklearn.pipeline import Pipeline, FunctionTransformer
from sklearn.compose import ColumnTransformer

from sklearn.metrics import accuracy_score
from sklearn.feature_extraction import FeatureHasher
import scipy.sparse as sp

from sentence_transformers import SentenceTransformer, models, util
from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
#pip install streamlit

"""#2. Importaci√≥n dataset de P2"""

path_save='/content/drive/MyDrive/TFM_FILES'
file_name='df_limpio.csv'

"""Esta carepta ser√° provisional para guardar diversos ficheros auxiliares:"""

save_path = os.path.join(path_save, "recommender_artifacts")

file_path = os.path.join(path_save, file_name)
if os.path.exists(file_path):
  df = pd.read_csv(file_path)
else:
  print(f"Archivo no encontrado: {file_path}")

df.columns.tolist()

if 'tmdb_id' not in df.columns and 'id' in df.columns:
    df = df.rename(columns={'id': 'tmdb_id'})

df = df.set_index('tmdb_id')

df.info(verbose=True)

df.head(2).T

df_rec=df.copy()

"""#3 Funciones para calcular el recomendador

##3.1 Calculo join features
"""

feature_list=df_rec.columns.tolist()
feature_list

def build_joined_features(
    df: pd.DataFrame,
    cols: Sequence[str] = feature_list,
) -> pd.Series:
    """Devuelve una Serie con los metadatos concatenados y normalizados.

    Args:
        df: DataFrame con las columnas de entrada.
        cols: Lista de columnas que se desean concatenar. Por defecto se usa
              la constante global `feature_list`.
    Returns:
        pd.Series de strings lista para alimentar al modelo de texto.
    """
    # Convertir NaNs a cadena vac√≠a y unir todo con un espacio
    text = (
        df[cols]
        .apply(lambda row: " ".join(str(valor) for valor in row if pd.notna(valor)), axis=1)
        .str.lower()
    )
    return text

"""##3.2 Embeddings

Funci√≥n para pasar a valor num√©rico el texto encadenado.
"""

def compute_embeddings(
    texts: Sequence[str],
    model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    batch_size: int = 512,
    to_numpy: bool = True,
    normalize: bool = True,
) -> np.ndarray:
    """Codifica una lista de textos usando un modelo pre‚Äëentrenado.

    Returns
    -------
    np.ndarray
        Array (num_samples, embedding_dim) normalizado.
    """
    model = SentenceTransformer(model_name)
    embeddings = model.encode(
        list(texts),
        batch_size=batch_size,
        show_progress_bar=True,
        convert_to_numpy=to_numpy,
        normalize_embeddings=normalize,
    )
    return embeddings.astype("float32")

"""##3.3 Similitud

Debido a tener m√°s de 600.000 pel√≠culas y despu√©s de investigar, encontramos que el √≠ndice FAISS es la forma m√°s r√°pida (al estar basado en C++) para obtener los veccinos m√°s pr√≥ximos.
"""

def build_faiss_index(embeddings: np.ndarray) -> faiss.IndexFlatIP:
    """Crea y devuelve un √≠ndice FAISS (coseno = producto interno tras normalizar)."""
    dim = embeddings.shape[1]
    index = faiss.IndexFlatIP(dim)
    index.add(embeddings)
    return index

"""##3.4 Obtener valores similares

Funci√≥n para calcualr los m√°s simialres. Para todas las pel√≠culas tarda m√°s de 30 minutos. Mejor guardarla y s√≥lo "entrenarla" de vez en cuando.
"""

def precalc_similars(
    index: faiss.IndexFlatIP,
    embeddings: np.ndarray,
    top_k: int = 5,
) -> np.ndarray:
    """Devuelve una matriz (num_pel√≠culas, top_k) con los √≠ndices de las pel√≠culas m√°s similares.

    Nota: solicitamos `top_k+1` porque la primera coincidencia suele ser la propia
    pel√≠cula, que se descarta.
    """
    distances, idxs = index.search(embeddings, top_k + 1)
    return idxs[:, 1:]

"""A√±adir estas columnas al dataframe original."""

def add_similar_columns(df: pd.DataFrame, idxs: np.ndarray) -> pd.DataFrame:
    """A√±ade columnas `similar_1` ‚Ä¶ `similar_N` al DataFrame original y las devuelve. Son los √≠ncides de cada pel√≠cula que ha encontrado"""
    out = df.copy()
    id_array = df.index.to_numpy()
    for j in range(idxs.shape[1]):
        out[f"similar_{j + 1}"] = id_array[idxs[:, j]]
    return out

"""##3.5 Funiones guardado y cargado"""

def save_artifacts(
    path: str,
    df: pd.DataFrame,
    embeddings: np.ndarray,
    index: faiss.Index,
) -> None:
    """Guarda DataFrame, embeddings y el √≠ndice FAISS en `path`."""
    os.makedirs(path, exist_ok=True)

    df.to_parquet(os.path.join(path, "df_rec.parquet"))
    np.save(os.path.join(path, "embeddings.npy"), embeddings)
    faiss.write_index(index, os.path.join(path, "faiss.index"))

def load_artifacts(path: str,
                   use_gpu: bool = False
    ) -> Tuple[pd.DataFrame, np.ndarray, faiss.Index]:

    """Carga los tres artefactos guardados en `save_artifacts`."""
    df = pd.read_parquet(os.path.join(path, "df_rec.parquet"))
    emb = np.load(os.path.join(path, "embeddings.npy"))
    index = faiss.read_index(os.path.join(path, "faiss.index"))
    if use_gpu and not isinstance(index, faiss.IndexGPU):
        res = faiss.StandardGpuResources()
        index = faiss.index_cpu_to_gpu(res, 0, index)
    return df, emb, index

"""#4. Funci√≥n para ejecutar el modelo

Funci√≥n m√°s simple que en funci√≥n del ya dataframe trabajado, te devuelve las pel√≠culas m√°s similares.
"""

def recommend(
    user_titles: Sequence[str],
    df: pd.DataFrame,
    index: faiss.IndexFlatIP,
    embeddings: np.ndarray,
    k: int = 5,
) -> pd.DataFrame:
    """Devuelve las *k* pel√≠culas m√°s afines al gusto de `user_titles`.

    Los t√≠tulos del usuario se buscan en la columna `tmdb_original_title`
    (min√∫sculas para evitar problemas de may√∫sculas).  Excluye siempre las
    pel√≠culas aportadas antes de devolver las recomendaciones.
    """
    if "tmdb_original_title" not in df.columns:
        raise KeyError(
            "El DataFrame no contiene la columna 'tmdb_original_title'. "
            "Aseg√∫rate de que el nombre sea correcto."
        )

    # 1) Localizar las pel√≠culas del usuario en el DataFrame
    titles_lower = df["tmdb_original_title"].astype(str).str.lower()
    mask = titles_lower.isin([t.lower() for t in user_titles])

    if not mask.any():
        raise ValueError(
            "Ninguno de los t√≠tulos proporcionados se encuentra en el cat√°logo."
        )

    user_idx = np.where(mask)[0]                       # posiciones 0‚Äëbased
    query_emb = embeddings[user_idx].mean(axis=0, keepdims=True)

    # 2) Buscar m√°s vecinos de los necesarios para poder descartar los ya vistos
    _, idxs = index.search(query_emb, k + len(user_idx))

    # 3) Eliminar los que el usuario ya ha indicado y recortar a k resultados
    seen = set(user_idx)
    rec_indices = [idx for idx in idxs[0] if idx not in seen][:k]

    # 4) Devolver las k pel√≠culas recomendadas como DataFrame limpio
    return df.iloc[rec_indices].reset_index(drop=True)

"""Funci√≥n alternativa en base a texto libre"""

def recommend_from_text(
    query_text: str,
    df: pd.DataFrame,
    index: faiss.Index,
    embeddings: np.ndarray,
    k: int = 5,
    model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
) -> pd.DataFrame:
    """Devuelve las *k* pel√≠culas m√°s parecidas a un texto libre (sin t√≠tulo).

    El texto se codifica con el **mismo modelo** usado para crear los embeddings
    del cat√°logo, garantizando la compatibilidad dimensional y sem√°ntica.
    """

    model = SentenceTransformer(model_name)
    model.max_seq_length = 128

    query_emb = model.encode([query_text], convert_to_numpy=True, normalize_embeddings=True)
    _, idxs = index.search(query_emb.astype("float32"), k)
    rec_indices = idxs[0].tolist()

    return df.iloc[rec_indices].reset_index(drop=True)

"""#5 Ejecuci√≥n

##5.1 Funciones "grandes"

-> S√ìLO ejecutar 1 vez a la semana.
"""

print("Construyendo columna joined_features‚Ä¶")
df_rec["joined_features"] = build_joined_features(df_rec)

print("Generando embeddings‚Ä¶")
emb = compute_embeddings(df_rec["joined_features"], batch_size=256)

print("Construyendo √≠ndice FAISS‚Ä¶")
ix = build_faiss_index(emb)

print("Pre‚Äëcalculando pel√≠culas similares y guardando en DataFrame‚Ä¶")
sim_idxs = precalc_similars(ix, emb, top_k=5)
df_rec = add_similar_columns(df_rec, sim_idxs)

print(f"Guardando artefactos en {save_path}‚Ä¶")
save_artifacts(save_path, df_rec, emb, ix)

"""##5.2. Funci√≥n peque√±a

Funci√≥n peque√±a: Se puede ejecutar cuando se quiera!!
"""

print("Cargando artefactos‚Ä¶")
df_loaded, emb_loaded, ix_loaded = load_artifacts(save_path, use_gpu=False)

test_films=[
    'Lost Highway',
    'Blue Velvet',
    'The Tree of Life',
    'Taxi Driver',
    'Casino'
]

recomendaciones = recommend(
        test_films,
        df_loaded,
        ix_loaded,
        emb_loaded,
        k=5,
    )
recomendaciones[["tmdb_original_title", "imdb_startyear", "imdb_director", "imdb_writer","tmdb_production_countries","combined_genres","runtime"]]

free_text="Estoy muy contento hoy en d√≠a y me gusta mucho Scorsese"

recs_text = recommend_from_text(
        free_text,
        df_loaded,
        ix_loaded,
        emb_loaded,
        k=5
    )

recs_text[["tmdb_original_title", "imdb_startyear", "imdb_director", "imdb_writer","tmdb_production_countries","combined_genres","runtime"]]

import streamlit as st
import numpy as np

# Simulamos un modelo de predicci√≥n (puedes cargar el tuyo real con joblib/pickle)
def modelo_simulado(x):
    return round((x[0] * 2 + x[1] * 0.5), 2)

st.title("üé¨ Predicci√≥n de Rentabilidad de Pel√≠culas")

presupuesto = st.number_input("Presupuesto ($)", min_value=0)
popularidad = st.slider("Popularidad (0-100)", 0, 100)

if st.button("Predecir"):
    resultado = modelo_simulado([presupuesto, popularidad])
    st.success(f"Rentabilidad estimada: {resultado}")

#streamlit run app.py
